# Data Model: Module 4 — Vision-Language-Action (VLA)

**Feature**: Module 4 — Vision-Language-Action (VLA)
**Created**: 2025-12-19
**Status**: Draft

## Overview

This document defines the key data models and structures for the Vision-Language-Action (VLA) educational module. Since this is primarily a documentation module for educational content, the "data models" refer to the conceptual data structures and information flows that students will learn about and implement.

## Core Entities

### 1. VoiceCommand
**Description**: Represents a voice command that has been processed by the speech recognition system

**Fields**:
- `id`: Unique identifier for the command
- `raw_audio`: Original audio data (for educational purposes)
- `transcribed_text`: Text output from speech recognition
- `intent`: Parsed intent from natural language processing
- `parameters`: Key-value pairs of extracted parameters
- `confidence`: Confidence score of the recognition (0.0-1.0)
- `timestamp`: When the command was received
- `status`: Current processing status (pending, processed, failed)

**Validation Rules**:
- `transcribed_text` must not be empty
- `confidence` must be between 0.0 and 1.0
- `timestamp` must be in the past

### 2. VisionInput
**Description**: Represents visual input from camera sensors processed by computer vision systems

**Fields**:
- `id`: Unique identifier for the vision input
- `image_data`: Image data from camera (for educational examples)
- `detected_objects`: List of objects detected in the scene
- `object_positions`: 3D positions of detected objects
- `scene_description`: Natural language description of the scene
- `timestamp`: When the image was captured
- `frame_rate`: Rate at which images are being processed

**Validation Rules**:
- `image_data` must be valid image format
- `timestamp` must be in the past
- `frame_rate` must be positive

### 3. CognitivePlan
**Description**: Represents a high-level plan generated by the LLM that combines vision and language inputs

**Fields**:
- `id`: Unique identifier for the plan
- `input_description`: Description of the inputs that generated this plan
- `goal`: The overall goal to be achieved
- `action_sequence`: Ordered list of actions to execute
- `confidence`: Confidence in the plan (0.0-1.0)
- `constraints`: Environmental or safety constraints
- `estimated_time`: Estimated time to complete the plan
- `status`: Current status of the plan (pending, executing, completed, failed)

**Validation Rules**:
- `action_sequence` must not be empty
- `confidence` must be between 0.0 and 1.0
- `estimated_time` must be positive

### 4. RobotAction
**Description**: Represents a specific action that can be executed by the robot

**Fields**:
- `id`: Unique identifier for the action
- `action_type`: Type of action (move, pick, place, speak, etc.)
- `parameters`: Parameters required for the action
- `priority`: Priority level (1-10)
- `estimated_duration`: Estimated time to complete the action
- `prerequisites`: Other actions that must complete first
- `safety_level`: Safety criticality (low, medium, high)

**Validation Rules**:
- `action_type` must be a valid action type
- `priority` must be between 1 and 10
- `estimated_duration` must be positive

### 5. VLAIntegration
**Description**: Represents the complete integration of Vision, Language, and Action systems

**Fields**:
- `id`: Unique identifier for the integration instance
- `voice_processor_status`: Status of the voice processing component
- `vision_processor_status`: Status of the vision processing component
- `cognitive_planner_status`: Status of the cognitive planning component
- `action_executor_status`: Status of the action execution component
- `overall_confidence`: Overall confidence in the integrated system
- `last_interaction_time`: Time of last system interaction
- `error_log`: Log of errors encountered

**Validation Rules**:
- `overall_confidence` must be between 0.0 and 1.0
- `last_interaction_time` must be in the past

## Data Flow Patterns

### Voice-to-Action Flow
```
VoiceCommand (raw_audio) → Speech Recognition → VoiceCommand (transcribed_text) →
NLP Processing → RobotAction → ROS 2 Command → Robot Execution
```

### Vision-Language Planning Flow
```
VisionInput (image_data) → Object Detection → VisionInput (detected_objects) →
LLM Processing → CognitivePlan → RobotAction → ROS 2 Command → Robot Execution
```

### Integrated VLA Flow
```
VoiceCommand + VisionInput → Cognitive Planning → CognitivePlan →
Action Sequencing → RobotAction(s) → ROS 2 Commands → Robot Execution
```

## State Transitions

### VoiceCommand States
- `pending` → `processing` → `processed` (success) or `failed` (error)
- `processed` → `executed` (when robot action is completed)

### CognitivePlan States
- `draft` → `validating` → `approved` → `executing` → `completed` or `failed`

### VLAIntegration States
- `initializing` → `ready` → `active` → `degraded` (if component fails) → `error`

## Relationships

- A `VLAIntegration` contains multiple `VoiceCommand`, `VisionInput`, and `CognitivePlan` instances
- A `CognitivePlan` contains multiple `RobotAction` instances
- A `VoiceCommand` may generate one or more `RobotAction` instances
- A `VisionInput` may influence one or more `CognitivePlan` instances

## Validation Requirements

### Data Integrity
- All timestamps must be in the past
- Confidence values must be between 0.0 and 1.0
- Required fields must not be empty

### Educational Requirements
- All data models must be simple enough for students to understand
- Examples must be practical and executable
- Data flow patterns must be clearly documented